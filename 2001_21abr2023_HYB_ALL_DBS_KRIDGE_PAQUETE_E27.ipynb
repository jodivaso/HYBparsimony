{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2adce046",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T18:33:46.824478Z",
     "start_time": "2023-04-21T18:33:46.803049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.3 1.5.2 1.23.5\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import glob, random, time, sys\n",
    "from string import printable\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.model_selection import RepeatedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso, RidgeClassifier, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error, log_loss, accuracy_score, mean_absolute_error\n",
    "from sklearn.datasets import load_diabetes, load_iris\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from HYBparsimony import Population, HYBparsimony, order\n",
    "from HYBparsimony.hybparsimony import default_cv_score_classification\n",
    "from HYBparsimony.util import knn_complexity\n",
    "\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# from GAparsimony import GAparsimony, Population, getFitness\n",
    "# from GAparsimony.util import linearModels_complexity, svm_complexity, mlp_complexity, randomForest_complexity\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "# from PSOparsimony_Nested import PSOparsimony\n",
    "from bayes_opt import BayesianOptimization\n",
    "# from HYBparsimony import HYBparsimony\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# from Hyb_nestedCV import HYB_NestedCV\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# comment in Beronia (not installed)\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "print(sklearn.__version__, pd.__version__, np.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0292298f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T18:34:02.193634Z",
     "start_time": "2023-04-21T18:33:47.568910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 0\n",
      "Current best score: -3004.3006761612755\n",
      "Running iteration 1\n",
      "Current best score: -2990.7795720943313\n",
      "Time limit reached. Stopped.\n",
      "Selected features: ['age' 'sex' 'bmi' 'bp' 's1' 's2' 's4' 's5' 's6']\n",
      "RMSE test 53.60827964891579\n"
     ]
    }
   ],
   "source": [
    "# REGRESIÓN\n",
    "# Cargo un dataset de regresión\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "###############################################################\n",
    "#                EJEMPLO OTRO SCORING Y OTRO CV               #\n",
    "###############################################################\n",
    "KNN_Model = {\"estimator\": KNeighborsRegressor,\n",
    "                 \"complexity\": knn_complexity,\n",
    "                 \"n_neighbors\": {\"range\": (1, 10), \"type\": Population.INTEGER}\n",
    "                 }\n",
    "HYBparsimony_model = HYBparsimony(scoring=\"neg_mean_squared_error\", \n",
    "                                  algorithm ='KernelRidge',\n",
    "                                  cv = RepeatedKFold(n_splits=10, n_repeats=5), \n",
    "                                  n_jobs = 1,\n",
    "                                  rerank_error = 0.1, \n",
    "                                  features = diabetes.feature_names)\n",
    "HYBparsimony_model.fit(X_train, y_train, time_limit=0.2)\n",
    "preds = HYBparsimony_model.predict(X_test)\n",
    "print(\"RMSE test\", mean_squared_error(y_test, preds, squared=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8905501",
   "metadata": {},
   "source": [
    "## Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fdb526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:11:51.498235Z",
     "start_time": "2023-04-12T14:11:51.419202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Argumentos\n",
    "# ----------\n",
    "def in_nb():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, '__file__')\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--method\", type=str, default=\"PSO\")\n",
    "parser.add_argument(\"--database\", type=str, default=\"boston_norm.csv\")\n",
    "parser.add_argument(\"--algorithm\", type=str, default=\"\")  # kridge, mlp, rf, svr\n",
    "parser.add_argument(\"--GPU\", type=str, default=\"False\")\n",
    "parser.add_argument(\"--nump\", type=int, default=40)\n",
    "parser.add_argument(\"--maxiter\", type=int, default=200)\n",
    "parser.add_argument(\"--early\", type=int, default=35)\n",
    "parser.add_argument(\"--pcrossover_elitists\", type=float, default=None)\n",
    "parser.add_argument(\"--pcrossover\", type=float, default=None)\n",
    "parser.add_argument(\"--Lambda\", type=float, default=1.0)\n",
    "parser.add_argument(\"--c1\", type=float, default=1.193)\n",
    "parser.add_argument(\"--c2\", type=float, default=1.193)\n",
    "parser.add_argument(\"--IWmax\", type=float, default=0.9)\n",
    "parser.add_argument(\"--IWmin\", type=float, default=0.4)\n",
    "parser.add_argument(\"--K\", type=int, default=3)\n",
    "parser.add_argument(\"--globthr\", type=float, default=1.0)\n",
    "parser.add_argument(\"--CVrep\", type=int, default=5)\n",
    "parser.add_argument(\"--CVspl\", type=int, default=5)\n",
    "parser.add_argument(\"--nruns\", type=int, default=5)\n",
    "parser.add_argument(\"--numseeds\", type=int, default=5)\n",
    "parser.add_argument(\"--timelimitmin\", type=float, default=1)\n",
    "parser.add_argument(\"--rerank\", type=float, default=0.001)\n",
    "parser.add_argument(\"--dirout\", type=str, default=\"respso2\")\n",
    "parser.add_argument(\"--not_muted\", type=int, default=3)\n",
    "parser.add_argument(\"--n_jobs\", type=int, default=1)\n",
    "parser.add_argument(\"--n_jobs_autogluon\", type=int, default=64)\n",
    "parser.add_argument(\"--temp_autogluon\", type=str, default=\"temp_autogluon\")\n",
    "parser.add_argument(\"--train_size\", type=int, default=2000)\n",
    "\n",
    "PRUEBA = 'HYB'\n",
    "if PRUEBA == 'HYB':\n",
    "    # PSO\n",
    "    cmdline = \"--method HYB --rerank 0.001 --nump 15 --numseeds 5 --pcrossover 0.10\"\n",
    "    cmdline += \" --c1 1.193 --c2 1.193 --maxiter 200 --n_jobs 1 --dirout hyb_1001_12abr23_e27 --algorithm kridge\"\n",
    "    cmdline = cmdline.split(' ')\n",
    "    \n",
    "if PRUEBA == 'PSO':\n",
    "    # PSO\n",
    "    cmdline = \"--method PSO --rerank 0.001 --nump 15 --numseeds 5 --pcrossover 0.10\"\n",
    "    cmdline += \" --c1 1.193 --c2 1.193 --maxiter 200 --n_jobs 1 --dirout pso_16_22mar23 --algorithm kridge\"\n",
    "    cmdline = cmdline.split(' ')\n",
    "    \n",
    "if PRUEBA == 'BAYESOPT':\n",
    "    # Bayesian Optimization\n",
    "    cmdline = \"--database BD_soldadura_20feb23.csv --method BAYESOPT --algorithm kridge\"\n",
    "    cmdline += \" --maxiter 200 --nruns 10 --dirout bayes_kridge_10runs --n_jobs 1\"\n",
    "    cmdline = cmdline.split(' ')\n",
    "\n",
    "if PRUEBA == 'CONSOLE':\n",
    "    cmdline = sys.argv[1:]  # Read from console\n",
    "\n",
    "arg_in = parser.parse_args(cmdline)\n",
    "str_todos = '_'.join(str(arg_in).split(' '))\n",
    "str_todos = str_todos.replace(',', '')\n",
    "str_todos = str_todos.replace('Namespace(', '')\n",
    "str_todos = str_todos.replace(')', '')\n",
    "str_todos = str_todos.replace('=', '_')\n",
    "str_todos = str_todos.replace('.', '_')\n",
    "str_todos = str_todos.replace(\"'\", \"\")\n",
    "str_todos = str_todos.replace('\\\\r', '')\n",
    "print(str_todos)\n",
    "\n",
    "# Decreasing curve of worst\n",
    "if arg_in.pcrossover != None and arg_in.pcrossover != 0.0:\n",
    "    perc_malos = 0.80 * np.exp(-arg_in.pcrossover * np.arange(arg_in.maxiter))\n",
    "    perc_malos[perc_malos < 0.10] = 0.10\n",
    "else:\n",
    "    perc_malos = None\n",
    "    \n",
    "if arg_in.GPU == 'True':\n",
    "    arg_in.GPU = True\n",
    "    IS_GPU = 'YES'\n",
    "else:\n",
    "    arg_in.GPU = False\n",
    "    IS_GPU = 'NO'\n",
    "print('Using GPU=', IS_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8932d2d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:11:51.512897Z",
     "start_time": "2023-04-12T14:11:51.502003Z"
    }
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y_%m_%d_%H_%M_%S_\")\n",
    "NAME_FILE = dt_string + '_' + arg_in.method + '__' + arg_in.algorithm + '__' + '__' + str(arg_in.nump)  # .join(arg_in[:2]) #+ str_todos\n",
    "NAME_FILE = ''.join(char for char in NAME_FILE if char in printable)\n",
    "print(repr(NAME_FILE))\n",
    "\n",
    "NUM_PARTICULAS = arg_in.nump\n",
    "DIR_SALIDA = str(arg_in.dirout).replace('\\\\r', '').replace('\\r', '')\n",
    "DIR_SALIDA = ''.join(char for char in DIR_SALIDA if char in printable)\n",
    "print(repr(DIR_SALIDA))\n",
    "\n",
    "if not os.path.exists(DIR_SALIDA):\n",
    "    os.mkdir(DIR_SALIDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279d67c7",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcbd332",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:11:51.532671Z",
     "start_time": "2023-04-12T14:11:51.519278Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def rmse_func(y_true, y_pred):\n",
    "    return -np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def decision_trees_complexity(model, nFeatures, **kwargs):\n",
    "    num_leaves = model.get_n_leaves()\n",
    "    int_comp = np.min((1E09 - 1, num_leaves))  # More leaves more complex\n",
    "    return nFeatures * 1E09 + int_comp\n",
    "\n",
    "def randomForest_complexity(model, nFeatures, **kwargs):\n",
    "    #     num_leaves = [tree.get_n_leaves() for tree in model.estimators_]\n",
    "    #     int_comp = np.min((1E09-1,np.mean(num_leaves))) # More leaves more complex\n",
    "    max_depth = model.get_params()['max_depth']\n",
    "    int_comp = max_depth  # Internal Complexity\n",
    "    return nFeatures * 1E09 + int_comp\n",
    "\n",
    "def kernel_ridge_complexity(model, nFeatures, **kwargs):\n",
    "    weights_l2 = np.sum(model.dual_coef_ ** 2)\n",
    "    int_comp = np.min((1E09 - 1, weights_l2))  # More leaves more complex\n",
    "    return nFeatures * 1E09 + int_comp\n",
    "\n",
    "def mlp_complexity_hidden(model, nFeatures, **kwargs):\n",
    "    weights = [np.concatenate(model.intercepts_)]\n",
    "    for wm in model.coefs_:\n",
    "        weights.append(wm.flatten())\n",
    "    weights = np.concatenate(weights) \n",
    "    int_comp = np.min((1E09-1,np.sum(weights**2)))\n",
    "#     int_comp = model.hidden_layer_sizes\n",
    "    return nFeatures*1E09 + int_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097779f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:11:51.547175Z",
     "start_time": "2023-04-12T14:11:51.535800Z"
    }
   },
   "outputs": [],
   "source": [
    "def fitness_model(cromosoma, X_train, y_train, X_val, y_val, iteration, particle, **kwargs):\n",
    "        global historical_todos, innerfold, outerfold, target_name\n",
    "        global algorithm, metric, complexity\n",
    "        # Todas las feats (hay que poner selec features en otros problemas)\n",
    "        num_feats = X_train.shape[1]\n",
    "        parametros = cromosoma.params.copy()\n",
    "        selection = cromosoma.columns\n",
    "        # train the model\n",
    "        if arg_in.algorithm == 'mlp':\n",
    "            parametros['alpha'] = 10.0**parametros['alpha']\n",
    "            parametros['hidden_layer_sizes'] = int(parametros['hidden_layer_sizes'])\n",
    "        \n",
    "        if arg_in.algorithm == 'kridge':\n",
    "            parametros['alpha'] = 10.0**parametros['alpha']\n",
    "            parametros['gamma'] = 10.0**parametros['gamma']\n",
    "\n",
    "        modelo = algorithm(**parametros).fit(X_train.values[:,selection], y_train)\n",
    "        fitness_train = metric(y_train.values.flatten(), modelo.predict(X_train.values[:,selection]))\n",
    "        fitness_val = metric(y_val.values.flatten(), modelo.predict(X_val.values[:,selection]))\n",
    "        fitness_test = metric(y_test.values.flatten(), modelo.predict(X_test.values[:,selection]))\n",
    "#         historical_todos.append(dict(target_name=target_name, outerfold=outerfold, innerfold=innerfold,\n",
    "#                                      iteration=iteration, num_indiv=particle, \n",
    "#                                      fitness_train= -fitness_train, fitness_val= -fitness_val, fitness_test= -fitness_test,\n",
    "#                                      num_feats=num_feats, parametros=parametros,\n",
    "#                                      feats = cromosoma.columns#, feats_prob = feats_prob.tolist()\n",
    "#                                      ))\n",
    "        return np.array([fitness_train, fitness_val, complexity(modelo, np.sum(cromosoma.columns))]), modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a628edc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:11:51.559086Z",
     "start_time": "2023-04-12T14:11:51.551346Z"
    }
   },
   "outputs": [],
   "source": [
    "# def getFitness_BayesOpt(algorithm, metric, complexity, cv=RepeatedKFold(n_splits=10, n_repeats=5, random_state=42),\n",
    "#                           n_jobs=-1):\n",
    "#          def fitness(X_train, X_val, y_train, y_val, cromosoma, **kwargs):\n",
    "#             global mejor_train, mejor_val, mejor_tst, parametros_mejor, num_indiv, columnas_mejor, target_name, historical_todos, run, numseeds, mejor_nfs\n",
    "#             global input_names, target_name, mejor_solucion_es, X_test, y_test\n",
    "\n",
    "#             X_train = X_train.values\n",
    "#             y_train = y_train.values\n",
    "#             X_val = X_val.values\n",
    "#             y_val = y_val.values\n",
    "#             data_train_model = X_train[:, cromosoma.columns]\n",
    "#             data_val_model = X_val[:, cromosoma.columns]\n",
    "\n",
    "#             num_feats = np.sum(cromosoma.columns)\n",
    "#             # train the model\n",
    "#             parametros = cromosoma.params.copy()\n",
    "#             print('Var=', target_name, 'Seed=', numseeds, 'Run=', run, 'Iter=', num_indiv // NUM_PARTICULAS, 'Nindiv=',\n",
    "#                   num_indiv % NUM_PARTICULAS,\n",
    "#                   'NFs=', num_feats, parametros)\n",
    "#             print(cromosoma.columns)\n",
    "#             print(arg_in.algorithm, ' : mejor_val=', mejor_val, 'mejor_tst=', mejor_tst, 'numfs=', mejor_nfs, 'params=',\n",
    "#                   parametros_mejor)\n",
    "\n",
    "#             #         try:\n",
    "#             aux = algorithm(**parametros)\n",
    "\n",
    "#             fitness_train = cross_val_score(aux, data_train_model, y_train, scoring=make_scorer(metric), cv=cv, n_jobs=n_jobs).mean()\n",
    "\n",
    "#             modelo = algorithm(**parametros).fit(data_train_model, y_train)\n",
    "#             fitness_val = metric(modelo.predict(data_val_model), y_val)\n",
    "#             # Esto es para ver cómo se comporta el individuo contra el otro 50% (la BD de test)\n",
    "#             fitness_test = metric(modelo.predict(X_test.values[:,cromosoma.columns]), y_test)\n",
    "\n",
    "#             print(\"FITNESS_VAL =\", fitness_val)\n",
    "#             if -fitness_val < mejor_val:\n",
    "#                 mejor_train = -fitness_train\n",
    "#                 mejor_val = -fitness_val\n",
    "#                 mejor_tst = -fitness_test\n",
    "#                 mejor_nfs = num_feats\n",
    "#                 parametros_mejor = parametros\n",
    "#                 columnas_mejor = cromosoma.columns\n",
    "#             print('fitness_val=', -fitness_val)\n",
    "#             mejor_solucion_es = arg_in.algorithm + ' : mejor_val=' + str(mejor_val) + ' mejor_tst=' + str(\n",
    "#                 mejor_tst) + 'numfs=' + str(mejor_nfs) + ' params=' + str(parametros_mejor)\n",
    "#             # Aquí divido entre 10 porque hay 10 puntos iniciales al hacer BayesOpt.\n",
    "#             historical_todos.append(dict(target_name=target_name, iteracion=num_indiv // 10,\n",
    "#                                          num_indiv=num_indiv % 10,\n",
    "#                                          mejor_train = mejor_train, mejor_val = mejor_val, mejor_test=mejor_tst,\n",
    "#                                          fitness_train=-fitness_train, fitness_val=-fitness_val, fitness_test=-fitness_test,\n",
    "#                                          num_feats=num_feats, parametros=parametros, numseeds=numseeds, run=run,\n",
    "#                                          mejor_solucion_es=mejor_solucion_es, feats = cromosoma.columns))\n",
    "#             num_indiv += 1\n",
    "#             #fitnessval es negativo. Y nos interesa que lo sea, porque el Bayesian Opt va a optimizar maximizando..\n",
    "#             return np.array(\n",
    "#                 [fitness_train, fitness_val, complexity(modelo, np.sum(cromosoma.columns))]), modelo, mejor_solucion_es\n",
    "\n",
    "#         #         except Exception as e:\n",
    "#         #             print(e)\n",
    "#         #             return np.array([np.NINF, np.NINF, np.Inf]), None\n",
    "#         return fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc6d05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:11:51.584239Z",
     "start_time": "2023-04-12T14:11:51.562855Z"
    }
   },
   "outputs": [],
   "source": [
    "def fitness_nested(algorithm_indiv, metric_indiv, complexity_indiv):\n",
    "    global algorithm, metric, complexity\n",
    "    if algorithm_indiv is None:\n",
    "        raise Exception(\"An algorithm function must be provided!!!\")\n",
    "    if metric_indiv is None or not callable(metric_indiv):\n",
    "        raise Exception(\"A metric function must be provided!!!\")\n",
    "    if complexity_indiv is None or not callable(complexity_indiv):\n",
    "        raise Exception(\"A complexity function must be provided!!!\")\n",
    "    algorithm = algorithm_indiv\n",
    "    metric = metric_indiv\n",
    "    complexity = complexity_indiv\n",
    "    return fitness_model\n",
    "\n",
    "def fitness_nested_for_parallel(algorithm_indiv, metric_indiv, complexity_indiv, cromosoma, X_train, y_train, X_val, y_val, iteration, particle):\n",
    "    global algorithm, metric, complexity\n",
    "    if algorithm_indiv is None:\n",
    "        raise Exception(\"An algorithm function must be provided!!!\")\n",
    "    if metric_indiv is None or not callable(metric_indiv):\n",
    "        raise Exception(\"A metric function must be provided!!!\")\n",
    "    if complexity_indiv is None or not callable(complexity_indiv):\n",
    "        raise Exception(\"A complexity function must be provided!!!\")\n",
    "    algorithm = algorithm_indiv\n",
    "    metric = metric_indiv\n",
    "    complexity = complexity_indiv\n",
    "    return fitness_model(cromosoma, X_train, y_train, X_val, y_val, iteration, particle, visualize=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b1d7a6",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a68905",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:11:51.603762Z",
     "start_time": "2023-04-12T14:11:51.588738Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "if arg_in.algorithm == 'kridge':\n",
    "    \n",
    "    fitness_fun = fitness_nested(KernelRidge, rmse_func, kernel_ridge_complexity)\n",
    "    params = {\"alpha\": {\"range\": (-7, 0), \"type\": Population.FLOAT},\n",
    "              \"gamma\": {\"range\": (-7, 0), \"type\": Population.FLOAT},\n",
    "              \"kernel\": {\"value\": \"rbf\", \"type\": Population.CONSTANT}}\n",
    "\n",
    "if arg_in.algorithm == 'mlp':\n",
    "    if arg_in.n_jobs<=1:\n",
    "        fitness_fun = fitness_nested(MLPRegressor, rmse_func, mlp_complexity_hidden)\n",
    "    else:\n",
    "        fitness_fun = partial(fitness_nested_for_parallel, \n",
    "                          MLPRegressor, \n",
    "                          rmse_func, \n",
    "                          mlp_complexity_hidden)\n",
    "    params = {\"hidden_layer_sizes\": {\"range\": (1, 25), \"type\": Population.INTEGER},  # MLPRegressor\n",
    "              \"alpha\": {\"range\": (-7, 0), \"type\": Population.FLOAT},\n",
    "              \"solver\": {\"value\": \"lbfgs\", \"type\": Population.CONSTANT},\n",
    "              \"activation\": {\"value\": \"logistic\", \"type\": Population.CONSTANT},\n",
    "              \"n_iter_no_change\": {\"value\": 20, \"type\": Population.CONSTANT},\n",
    "              \"tol\": {\"value\": 1e-5, \"type\": Population.CONSTANT},\n",
    "              \"random_state\": {\"value\": 1234, \"type\": Population.CONSTANT},\n",
    "              \"max_iter\": {\"value\": 5000, \"type\": Population.CONSTANT}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3691e",
   "metadata": {},
   "source": [
    "## Search Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76f187",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T14:11:51.634218Z",
     "start_time": "2023-04-12T14:11:51.607741Z"
    }
   },
   "outputs": [],
   "source": [
    "def search_model(output_var, seed_ini=1234, X_train=None, X_val=None, y_train=None, y_val=None,\n",
    "                 X_train_val = None, y_train_val = None, features = None):\n",
    "\n",
    "    global X_train_orig, X_test_orig, input_names, target_name, X, y\n",
    "    global fitness_fun, params, arg_in\n",
    "    if arg_in.method == 'PSO' or arg_in.method == 'HYB':\n",
    "        PSOparsimony_model = PSOparsimony(fitness=fitness_fun,\n",
    "                                          params=params,\n",
    "                                          features=input_names,\n",
    "                                          keep_history=True,\n",
    "                                          rerank_error=arg_in.rerank,\n",
    "                                          npart=arg_in.nump,\n",
    "                                          maxiter=arg_in.maxiter,\n",
    "                                          early_stop=arg_in.early,\n",
    "                                          feat_thres=0.90,  # Perc selected features in first generation\n",
    "                                          seed_ini=seed_ini,\n",
    "                                          Lambda=arg_in.Lambda,\n",
    "                                          c1=arg_in.c1,\n",
    "                                          c2=arg_in.c2,\n",
    "                                          IW_max=arg_in.IWmax,\n",
    "                                          IW_min=arg_in.IWmin,\n",
    "                                          K=arg_in.K,\n",
    "                                          best_global_thres=arg_in.globthr,\n",
    "                                          # pcrossover_elitists=arg_in.pcrossover_elitists,\n",
    "                                          pcrossover=perc_malos,\n",
    "                                          n_jobs=arg_in.n_jobs,\n",
    "                                          verbose=0)\n",
    "        PSOparsimony_model.fit(X_train = X_train[features], y_train = y_train,\n",
    "                               X_val = X_val[features], y_val = y_val)\n",
    "        return PSOparsimony_model\n",
    "    \n",
    "    if arg_in.method == 'BAYESOPT':\n",
    "        global NUM_PARTICULAS\n",
    "        NUM_PARTICULAS = arg_in.maxiter\n",
    "\n",
    "        pbounds = dict()\n",
    "        for parm in params:\n",
    "            if params[parm]['type'] <= 1:\n",
    "                pbounds[parm] = params[parm]['range']\n",
    "\n",
    "        if arg_in.algorithm == 'kridge':\n",
    "            def optimize_fun(alpha, gamma):\n",
    "                chromosome_all_ones = pd.Series(\n",
    "                    dict(columns=(np.ones(len(input_names)) == 1), params=dict(alpha=alpha, gamma=gamma, kernel=\"rbf\")))\n",
    "                res = fitness_fun(chromosome_all_ones, X_train, y_train, X_val, y_val, 0, 0)\n",
    "                return res[0][1] # Antes era res[0][0], pero ahora nos quedamos con res[0][1] porque queremos el error en X_val.\n",
    "        if arg_in.algorithm == 'mlp':\n",
    "            def optimize_fun(hidden_layer_sizes, alpha):\n",
    "                params_opt = dict(hidden_layer_sizes=int(hidden_layer_sizes),\n",
    "                                  alpha=alpha,\n",
    "                                  solver='lbfgs',\n",
    "                                  activation='logistic',\n",
    "                                  n_iter_no_change=20,\n",
    "                                  tol=1e-5,\n",
    "                                  random_state=1234,\n",
    "                                  max_iter=5000)\n",
    "                chromosome_all_ones = pd.Series(dict(columns=(np.ones(len(input_names)) == 1), params=params_opt))\n",
    "                res = fitness_fun(chromosome_all_ones, X_train, y_train, X_val, y_val, 0, 0)\n",
    "                return res[0][1] # Antes era res[0][0], pero ahora nos quedamos con res[0][1] porque queremos el error en X_val.\n",
    "\n",
    "        # Optimize with BayesianOptimization\n",
    "        tic = time.time()\n",
    "        optimizer = BayesianOptimization(f=optimize_fun,\n",
    "                                         pbounds=pbounds,\n",
    "                                         random_state=seed_ini,\n",
    "                                         allow_duplicate_points=True) # Añado allow_duplicate_points=True para evitar errores que salían.\n",
    "        optimizer.maximize(init_points=10, n_iter=arg_in.maxiter - 10)\n",
    "        # Keep elapsed time in minutes\n",
    "        tac = time.time()\n",
    "        elapsed_time = (tac - tic) / 60.0\n",
    "        print('Elapsed_time:', elapsed_time)\n",
    "        print('Best:', optimizer.max)\n",
    "        \n",
    "        # Best model\n",
    "        # ----------\n",
    "        # KRIDGE\n",
    "        if arg_in.algorithm == 'kridge':\n",
    "            alpha = optimizer.max['params']['alpha']\n",
    "            gamma = optimizer.max['params']['gamma']\n",
    "            params_opt = dict(alpha=alpha, gamma=gamma, kernel=\"rbf\")\n",
    "            params_indiv = [optimizer.max['params']['alpha'], optimizer.max['params']['gamma']]\n",
    "\n",
    "        # MLP\n",
    "        if arg_in.algorithm == 'mlp':\n",
    "            hidden_layer_sizes = optimizer.max['params']['hidden_layer_sizes']\n",
    "            alpha = optimizer.max['params']['alpha']\n",
    "            params_opt = dict(hidden_layer_sizes=int(hidden_layer_sizes),\n",
    "                              alpha=alpha,\n",
    "                              solver='lbfgs',\n",
    "                              activation='logistic',\n",
    "                              n_iter_no_change=20,\n",
    "                              tol=1e-5,\n",
    "                              random_state=1234,\n",
    "                              max_iter=5000)\n",
    "            params_indiv = [optimizer.max['params']['hidden_layer_sizes'], optimizer.max['params']['alpha']]\n",
    "\n",
    "        # Return results with the best model\n",
    "        chromosome_all_ones = pd.Series(dict(columns=(np.ones(len(input_names)) == 1), params=params_opt))\n",
    "        res = fitness_fun(chromosome_all_ones, X_train, y_train, X_val, y_val, 0, 0)\n",
    "        return pd.Series(dict(best_model=res[1],\n",
    "                              bestsolution=np.concatenate([np.array(res[0]), params_indiv, np.ones(len(input_names))]),\n",
    "                              minutes_total=elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9fc7b1",
   "metadata": {},
   "source": [
    "## Search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b76aa",
   "metadata": {},
   "source": [
    "### Search with Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c4f056",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T02:19:01.688213Z",
     "start_time": "2023-04-12T14:11:51.638118Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tiempos = [['slice_norm_reduc.csv', 3, 2000, 378, 0.50], \n",
    "#            ['blog_norm.csv', 3, 2000, 276, 0.50], \n",
    "           ['crime_norm.csv', 3, 2000, 127, 0.50], \n",
    "#            ['tecator_norm.csv', 3, 2000, 124, 0.50],\n",
    "           ['ailerons_norm.csv', 3, 2000, 40, 0.50], \n",
    "#            ['puma_norm.csv', 3, 2000, 32, 0.50],\n",
    "           ['bank_norm.csv', 3, 2000, 32, 0.50], \n",
    "#            ['pol_norm.csv', 3, 2000, 26, 0.50],\n",
    "           \n",
    "#            ['cpu_act_norm.csv', 3, 2000, 21, 0.50],\n",
    "#            ['elevators_norm.csv', 3, 2000, 18, 0.50],\n",
    "#            ['meta_norm.csv', 3, 2000, 17, 0.50],\n",
    "#            ['bodyfat_norm.csv', 3, 2000, 14, 0.50], \n",
    "#            ['boston_norm.csv', 3, 2000, 13, 0.50],\n",
    "#            ['housing_norm.csv', 3, 2000, 13, 0.50],\n",
    "#            ['concrete_norm.csv', 3, 2000, 8, 0.50],\n",
    "#            ['no2_norm.csv', 3, 2000, 7, 0.50],\n",
    "#            ['pm10_norm.csv', 3, 2000, 7, 0.50],\n",
    "#            ['strike_norm.csv', 3, 2000, 6, 0.50]\n",
    "          ]\n",
    "    \n",
    "\n",
    "# # From Lee_Synthetic_All_10runs_27mar.ipynb\n",
    "# tiempos = [['ailerons_norm.csv', 3, 2000, 40, 1.0],\n",
    "#            ['crime_norm.csv', 3, 2000, 127, 0.4], \n",
    "#            ['blog_norm.csv', 3, 2000, 276, 0.6],\n",
    "#            ['slice_norm_reduc.csv', 3, 2000, 378, 0.60]]\n",
    "    \n",
    "\n",
    "run = 0\n",
    "for names_tiempo in tiempos:\n",
    "    name_db = names_tiempo[0].split('_')[0]\n",
    "    list_csv = os.listdir('.')\n",
    "    list_csv = [i for i in list_csv if f'{name_db}' in i]\n",
    "    file_db = list_csv[0]\n",
    "    time_limit = int(names_tiempo[1] * 60)  # Time limit in secs\n",
    "    \n",
    "    if arg_in.method == 'PSO':\n",
    "        perc_malos = None\n",
    "    else:\n",
    "        # Calcula la curva de perc_malos\n",
    "        perc_malos = 0.80 * np.exp(-names_tiempo[4] * np.arange(arg_in.maxiter))\n",
    "        perc_malos[perc_malos < 0.10] = 0.10\n",
    "        print(perc_malos)\n",
    "    \n",
    "#     train_val_size = names_tiempo[2]\n",
    "\n",
    "    df_VAL = pd.read_csv(file_db)\n",
    "    print(df_VAL.shape)\n",
    "    train_val_size = df_VAL.shape[0]//2\n",
    "    if train_val_size>2000:\n",
    "        train_val_size = 2000\n",
    "    names_tiempo[2] = train_val_size\n",
    "    print(name_db, time_limit, train_val_size)\n",
    "    \n",
    "    res_modelos = []\n",
    "    GA_models = []\n",
    "    historical_todos = []\n",
    "    tiempo_inicial = time.time()\n",
    "    outer_res = []\n",
    "    \n",
    "    NAME_FILE = dt_string + '_' + arg_in.method + '__' + arg_in.algorithm + '__' + file_db + '__' + str(arg_in.nump)  # .join(arg_in[:2]) #+ str_todos\n",
    "    NAME_FILE = ''.join(char for char in NAME_FILE if char in printable)\n",
    "    print(repr(NAME_FILE))\n",
    "    \n",
    "    for num_seed in range(arg_in.numseeds):\n",
    "        print('Processando Base de Datos con num_seed=', num_seed)\n",
    "        df_VAL = pd.read_csv(file_db)\n",
    "        VAL = df_VAL.copy()\n",
    "        scaler_pruebas = StandardScaler()\n",
    "        VAL_norm = pd.DataFrame(scaler_pruebas.fit_transform(VAL), columns=VAL.columns)\n",
    "\n",
    "        TST_RMSE_ITER = []\n",
    "        NUM_FEATS = []\n",
    "        \n",
    "        # Con selección\n",
    "        input_names = VAL_norm.columns[:-1]\n",
    "        target_name = VAL_norm.columns[-1]\n",
    "\n",
    "        X = VAL_norm[input_names]\n",
    "        y = VAL_norm[target_name]\n",
    "        seed_everything(num_seed)\n",
    "\n",
    "         # IMPORTANTE: En KFOLD_HYB_AUTOGLUON cojo 2000 filas de train_val y el resto de test...salvo que se especifique\n",
    "        # el parámetro train_size (que será el número de filas a pillar de la BD).\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, train_size = names_tiempo[2] / len(X), \n",
    "                                                                    random_state=12345)\n",
    "        size_train_val = len(X_train_val)\n",
    "        size_test = len(X_test)\n",
    "\n",
    "        model_name = 'KFOLD_HYB_AUTOGLUON'\n",
    "        selec_input_folds=[]\n",
    "        VAL_RMSE_ACC=[]\n",
    "        # En este caso, hacemos los X folds (fold es el numseeds)\n",
    "        kf = KFold(n_splits=arg_in.numseeds, shuffle=False)\n",
    "        for innerfold, (train_index, val_index) in enumerate(kf.split(X_train_val)):\n",
    "            print(f\"Executing num_seed:{num_seed} innerfold:{innerfold}:\")\n",
    "            X_train = X_train_val.iloc[train_index]\n",
    "            y_train = y_train_val.iloc[train_index]\n",
    "            X_val = X_train_val.iloc[val_index]\n",
    "            y_val = y_train_val.iloc[val_index]\n",
    "\n",
    "            res = []\n",
    "            mejor_train = np.inf\n",
    "            mejor_val = np.inf\n",
    "            mejor_tst = np.inf\n",
    "            mejor_nfs = np.inf\n",
    "            parametros_mejor = []\n",
    "            mejor_solucion_es = 'EMPTY'\n",
    "            columnas_mejor = []\n",
    "            num_indiv = 0\n",
    "\n",
    "            # Busco el mejor modelo para innerfold\n",
    "            res_search_model = search_model(target_name, seed_ini=1234 * (innerfold + 1) * (num_seed + 1) * (run+1),\n",
    "                                            X_train=X_train, y_train=y_train, \n",
    "                                            X_val=X_val, y_val=y_val,\n",
    "                                            features=input_names)\n",
    "\n",
    "            # Extrae Resultados del mejor modelo de cada innerfold\n",
    "            time_limit = int(arg_in.timelimitmin * 60)  # Time limit in secs\n",
    "            selec_input = input_names[res_search_model.bestsolution[-len(input_names):].astype(float) >= 0.50]\n",
    "            print(\"SELECT INPUT\", selec_input)\n",
    "            selec_input_folds.append(selec_input)\n",
    "\n",
    "            # ERRORES DE ENTRENAMIENTO, VALIDACIÓN Y TESTEO DE CADA FOLD:            \n",
    "            # Extrae Resultados mejor Modelo\n",
    "            model_name = str(res_search_model.best_model).split('(')[0] + '_' + target_name\n",
    "            model = res_search_model.best_model\n",
    "\n",
    "            # Entrenamos con TRAIN y medimos el error con VAL\n",
    "            model.fit(X_train[selec_input].values, y_train.values)\n",
    "            y_val_pred = model.predict(X_val[selec_input].values)\n",
    "            VAL_RMSE_FOLD = mean_squared_error(y_val.values, y_val_pred, squared=False)\n",
    "            VAL_MAE = mean_absolute_error(y_val.values, y_val_pred)\n",
    "            VAL_MAPE = mean_absolute_percentage_error(y_val.values, y_val_pred) * 100\n",
    "            y_test_pred = model.predict(X_test[selec_input].values)\n",
    "            TST_RMSE_FOLD = mean_squared_error(y_test.values, y_test_pred, squared=False)\n",
    "\n",
    "            # Entrenamos con TRAIN+VAL y medimos el error con TST\n",
    "            model.fit(X_train_val[selec_input].values, y_train_val.values)\n",
    "            y_test_pred = model.predict(X_test[selec_input].values)\n",
    "            TST_RMSE = mean_squared_error(y_test.values, y_test_pred, squared=False)\n",
    "            TST_MAE = mean_absolute_error(y_test.values, y_test_pred)\n",
    "            TST_MAPE = mean_absolute_percentage_error(y_test.values, y_test_pred) * 100\n",
    "\n",
    "            resultados_iter = dict(method=arg_in.method,\n",
    "                                   database=arg_in.database,\n",
    "                                   namevar=target_name,\n",
    "                                   size_train_val=size_train_val,\n",
    "                                   size_test=size_test,\n",
    "                                   num_input_feats=X.shape[1],\n",
    "                                   num_seed=num_seed,\n",
    "                                   innerfold=innerfold,\n",
    "                                   seed=num_seed, run=run, model_name=model_name,\n",
    "                                   rerank_error=arg_in.rerank,\n",
    "\n",
    "                                   VAL_RMSE_FOLD=VAL_RMSE_FOLD,\n",
    "                                   VAL_MAE=VAL_MAE,\n",
    "                                   VAL_MAPE=VAL_MAPE,\n",
    "                                   TST_RMSE_FOLD=TST_RMSE_FOLD, #rmse with train\n",
    "                                   TST_RMSE=TST_RMSE, #rmse with train+val\n",
    "                                   TST_MAE=TST_MAE,\n",
    "                                   TST_MAPE=TST_MAPE,\n",
    "\n",
    "                                   COMPLEXITY=res_search_model.bestsolution[2],\n",
    "                                   time=res_search_model.minutes_total,\n",
    "                                   last_iter=res_search_model.iteration,\n",
    "                                   mejor_val=mejor_val,\n",
    "                                   numindiv=arg_in.nump,\n",
    "                                   maxiter=arg_in.maxiter,\n",
    "                                   early=arg_in.early,\n",
    "                                   Lambda=arg_in.Lambda,\n",
    "                                   c1=arg_in.c1,\n",
    "                                   c2=arg_in.c2,\n",
    "                                   IWmax=arg_in.IWmax,\n",
    "                                   IWmin=arg_in.IWmin,\n",
    "                                   K=arg_in.K,\n",
    "                                   globthr=arg_in.globthr,\n",
    "                                   CVrep=arg_in.CVrep,\n",
    "                                   CVspl=arg_in.CVspl,\n",
    "                                   nruns=arg_in.nruns,\n",
    "                                   pcrossover_elitists=arg_in.pcrossover_elitists,\n",
    "                                   pcrossover=arg_in.pcrossover,\n",
    "                                   best_solution=res_search_model.bestsolution\n",
    "                                   )\n",
    "\n",
    "            # Escribimos en cada iteración en el CSV los resultados\n",
    "            res_modelos.append(resultados_iter)\n",
    "            res_final = pd.DataFrame(res_modelos)\n",
    "            res_final.to_csv(DIR_SALIDA + '/' + NAME_FILE + '_results.csv', index=False)\n",
    "\n",
    "        # FINAL MODEL FOR EACH OUTER WITH MEAN PARAMS\n",
    "        if arg_in.algorithm == 'mlp':\n",
    "            only_outer = res_final.query('num_seed==@num_seed').reset_index(drop=True)\n",
    "            only_outer['complex_best'] =  only_outer['best_solution'].apply(lambda x: x[2]).values\n",
    "            only_outer['hidden'] =  only_outer['best_solution'].apply(lambda x: x[3]).values\n",
    "            only_outer['alpha'] =  only_outer['best_solution'].apply(lambda x: 10**x[4]).values\n",
    "            mean_outer = only_outer[['VAL_RMSE_FOLD', 'VAL_MAE', 'VAL_MAPE', 'TST_RMSE_FOLD',\n",
    "                                     'TST_RMSE', 'TST_MAE', 'TST_MAPE',\n",
    "                                    'complex_best', 'hidden', 'alpha']].aggregate(['mean','std'])\n",
    "            fil0 = mean_outer.iloc[0]\n",
    "            fil0.index = ['mean_'+col for col in mean_outer.columns]\n",
    "            fil1 = mean_outer.iloc[1]\n",
    "            fil1.index = ['std_'+col for col in mean_outer.columns]\n",
    "            mean_outer = pd.concat(\n",
    "                [only_outer[['method', 'database', 'namevar', 'size_train_val', 'size_test',\n",
    "                       'num_input_feats', 'num_seed', 'innerfold', 'run', 'model_name',\n",
    "                       'rerank_error', 'time', 'last_iter', 'mejor_val',\n",
    "                       'numindiv', 'maxiter', 'early', 'Lambda', 'c1', 'c2', 'IWmax', 'IWmin',\n",
    "                       'K', 'globthr', 'CVrep', 'CVspl', 'nruns', 'pcrossover_elitists',\n",
    "                       'pcrossover', 'best_solution']].head(1),\n",
    "                 pd.DataFrame(pd.concat([fil0,fil1])).transpose()],axis=1)\n",
    "\n",
    "            # Model with mean of hyperparameters\n",
    "            # Entrenamos con TRAIN+VAL y medimos el error con TST\n",
    "            model = res_search_model.best_model\n",
    "            model.hidden_layer_sizes = int(mean_outer['mean_hidden'].values[0])\n",
    "            model.alpha = mean_outer['mean_alpha'].values[0]\n",
    "\n",
    "        if arg_in.algorithm == 'kridge':\n",
    "            only_outer = res_final.query('num_seed==@num_seed').reset_index(drop=True)\n",
    "            only_outer['complex_best'] =  only_outer['best_solution'].apply(lambda x: x[2]).values\n",
    "            only_outer['alpha'] =  only_outer['best_solution'].apply(lambda x: 10**x[3]).values\n",
    "            only_outer['gamma'] =  only_outer['best_solution'].apply(lambda x: 10**x[4]).values\n",
    "            mean_outer = only_outer[['VAL_RMSE_FOLD', 'VAL_MAE', 'VAL_MAPE', 'TST_RMSE_FOLD',\n",
    "                                     'TST_RMSE', 'TST_MAE', 'TST_MAPE',\n",
    "                                    'complex_best', 'alpha', 'gamma']].aggregate(['mean','std'])\n",
    "            fil0 = mean_outer.iloc[0]\n",
    "            fil0.index = ['mean_'+col for col in mean_outer.columns]\n",
    "            fil1 = mean_outer.iloc[1]\n",
    "            fil1.index = ['std_'+col for col in mean_outer.columns]\n",
    "            mean_outer = pd.concat(\n",
    "                [only_outer[['method', 'database', 'namevar', 'size_train_val', 'size_test',\n",
    "                       'num_input_feats', 'num_seed', 'innerfold', 'run', 'model_name',\n",
    "                       'rerank_error', 'time', 'last_iter', 'mejor_val',\n",
    "                       'numindiv', 'maxiter', 'early', 'Lambda', 'c1', 'c2', 'IWmax', 'IWmin',\n",
    "                       'K', 'globthr', 'CVrep', 'CVspl', 'nruns', 'pcrossover_elitists',\n",
    "                       'pcrossover', 'best_solution']].head(1),\n",
    "                 pd.DataFrame(pd.concat([fil0,fil1])).transpose()],axis=1)\n",
    "\n",
    "            # Model with mean of hyperparameters\n",
    "            # Entrenamos con TRAIN+VAL y medimos el error con TST\n",
    "            model = res_search_model.best_model\n",
    "            model.alpha = mean_outer['mean_alpha'].values[0]\n",
    "            model.gamma = mean_outer['mean_gamma'].values[0]\n",
    "\n",
    "        model.fit(X_train_val[selec_input].values, y_train_val.values)\n",
    "        y_test_pred = model.predict(X_test[selec_input].values)\n",
    "        OUTER_RMSE = mean_squared_error(y_test.values, y_test_pred, squared=False)\n",
    "        OUTER_MAE = mean_absolute_error(y_test.values, y_test_pred)\n",
    "        OUTER_MAPE = mean_absolute_percentage_error(y_test.values, y_test_pred) * 100\n",
    "        mean_outer['OUTER_RMSE'] = OUTER_RMSE\n",
    "        mean_outer['OUTER_MAE'] = OUTER_MAE\n",
    "        mean_outer['OUTER_MAPE'] = OUTER_MAPE\n",
    "        outer_res.append(mean_outer)\n",
    "    outer_res = pd.concat(outer_res)\n",
    "    outer_res.to_csv(DIR_SALIDA + '/' + NAME_FILE + '_' + name_db.split('_')[0] + '_outer.csv', index=False)\n",
    "    print(name_db)\n",
    "    display(outer_res[['mean_VAL_RMSE_FOLD', 'mean_TST_RMSE_FOLD', 'mean_TST_RMSE', 'OUTER_RMSE']].mean())\n",
    "    print('##########################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b51d1eb",
   "metadata": {},
   "source": [
    "## Obtain Outer Models by Selecting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f531d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T07:39:27.511997Z",
     "start_time": "2023-04-13T07:39:27.500475Z"
    }
   },
   "outputs": [],
   "source": [
    "path_files = 'hyb_1001_12abr23_e27/'\n",
    "archivos = os.listdir(path_files)\n",
    "archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd320b7f",
   "metadata": {},
   "source": [
    "### Bayesian with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da629615",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T07:39:30.364902Z",
     "start_time": "2023-04-13T07:39:30.351659Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimize with BayesianOptimization\n",
    "def bayes_cv(chromosome_all_ones):\n",
    "    global X_train_val, y_train_val\n",
    "    res_folds = []\n",
    "    kf = KFold(n_splits=arg_in.numseeds, shuffle=True, random_state=1234)\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(X_train_val)):\n",
    "        X_train = X_train_val.iloc[train_index].reset_index(drop=True)\n",
    "        y_train = y_train_val.iloc[train_index].reset_index(drop=True)\n",
    "        X_val = X_train_val.iloc[val_index].reset_index(drop=True)\n",
    "        y_val = y_train_val.iloc[val_index].reset_index(drop=True)\n",
    "        res_folds.append(fitness_fun(chromosome_all_ones, X_train, y_train, X_val, y_val, 0, 0)[0][1])\n",
    "    res_folds = np.array(res_folds)\n",
    "    return np.mean(res_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918b4be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T07:38:06.865637Z",
     "start_time": "2023-04-13T07:38:06.856139Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_fun(hidden_layer_sizes, alpha):\n",
    "    params_opt = dict(hidden_layer_sizes=int(hidden_layer_sizes),\n",
    "                      alpha=alpha,\n",
    "                      solver='lbfgs',\n",
    "                      activation='logistic',\n",
    "                      n_iter_no_change=20,\n",
    "                      tol=1e-5,\n",
    "                      random_state=1234,\n",
    "                      max_iter=5000)\n",
    "    chromosome_all_ones = pd.Series(dict(columns=(np.ones(len(input_names)) == 1), params=params_opt))\n",
    "    return bayes_cv(chromosome_all_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d0e8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T07:40:26.854950Z",
     "start_time": "2023-04-13T07:40:26.833459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ridge\n",
    "if arg_in.algorithm == 'ridge':\n",
    "    def optimize_fun(alpha):\n",
    "        chromosome_all_ones = pd.Series(\n",
    "            dict(columns=(np.ones(len(input_names)) == 1), params=dict(alpha=alpha, tol=1e-4)))\n",
    "        return bayes_cv(chromosome_all_ones)  \n",
    "    \n",
    "# KernelRidge\n",
    "if arg_in.algorithm == 'kridge':\n",
    "    def optimize_fun(alpha, gamma):\n",
    "        chromosome_all_ones = pd.Series(\n",
    "            dict(columns=(np.ones(len(input_names)) == 1), params=dict(alpha=alpha, gamma=gamma, kernel=\"rbf\")))\n",
    "        return bayes_cv(chromosome_all_ones)  # Antes era res[0][0], pero ahora nos quedamos con res[0][1] porque queremos el error en X_val.\n",
    "\n",
    "# SVR\n",
    "if arg_in.algorithm == 'svr':\n",
    "    def optimize_fun(C, gamma):\n",
    "        chromosome_all_ones = pd.Series(\n",
    "            dict(columns=(np.ones(len(input_names)) == 1), params=dict(C=C, gamma=gamma, kernel=\"rbf\")))\n",
    "        return bayes_cv(chromosome_all_ones)  # Antes era res[0][0], pero ahora nos quedamos con res[0][1] porque queremos el error en X_val.\n",
    "    \n",
    "# DecisionTreeRegressor\n",
    "if arg_in.algorithm == 'dtr':\n",
    "    def optimize_fun(max_depth, min_impurity_decrease):\n",
    "        chromosome_all_ones = pd.Series(\n",
    "            dict(columns=(np.ones(len(input_names)) == 1), params=dict(max_depth=max_depth, min_impurity_decrease=min_impurity_decrease)))\n",
    "        return bayes_cv(chromosome_all_ones)  # Antes era res[0][0], pero ahora nos quedamos con res[0][1] porque queremos el error en X_val.   \n",
    "    \n",
    "# RandomForest\n",
    "if arg_in.algorithm == 'rf':\n",
    "    def optimize_fun(max_depth, n_estimators, min_impurity_decrease):\n",
    "        chromosome_all_ones = pd.Series(\n",
    "            dict(columns=(np.ones(len(input_names)) == 1), params=dict(max_depth=max_depth, \n",
    "                                                                       n_estimators=n_estimators,\n",
    "                                                                       min_impurity_decrease=min_impurity_decrease)))\n",
    "        return bayes_cv(chromosome_all_ones)  # Antes era res[0][0], pero ahora nos quedamos con res[0][1] porque queremos el error en X_val.        \n",
    "\n",
    "# MultilayerPerceptron\n",
    "if arg_in.algorithm == 'mlp':\n",
    "    def optimize_fun(hidden_layer_sizes, alpha):\n",
    "        params_opt = dict(hidden_layer_sizes=int(hidden_layer_sizes),\n",
    "                          alpha=alpha,\n",
    "                          solver='lbfgs',\n",
    "                          activation='logistic',\n",
    "                          n_iter_no_change=20,\n",
    "                          tol=1e-5,\n",
    "                          random_state=1234,\n",
    "                          max_iter=5000)\n",
    "        chromosome_all_ones = pd.Series(dict(columns=(np.ones(len(input_names)) == 1), params=params_opt))\n",
    "        return bayes_cv(chromosome_all_ones) # Antes era res[0][0], pero ahora nos quedamos con res[0][1] porque queremos el error en X_val.\n",
    "\n",
    "def desnorm_y(y_pred):\n",
    "    return ((y_pred*y_orig_sqrt_std)+y_orig_sqrt_mean)**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2604bd6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T18:05:33.491282Z",
     "start_time": "2023-04-14T08:00:55.106142Z"
    }
   },
   "outputs": [],
   "source": [
    "for thr_features in [0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70]:\n",
    "    run = 0\n",
    "    features_list = []\n",
    "    for names_tiempo in tiempos:\n",
    "        name_db = names_tiempo[0].split('_')[0]\n",
    "        list_csv = os.listdir('.')\n",
    "        list_csv = [i for i in list_csv if f'{name_db}' in i]\n",
    "        file_db = list_csv[0]\n",
    "        time_limit = int(names_tiempo[1] * 60)  # Time limit in secs\n",
    "        train_val_size = names_tiempo[2]\n",
    "        print(name_db, time_limit, train_val_size)\n",
    "\n",
    "        NAME_FILE = dt_string + '_' + arg_in.method + '__' + arg_in.algorithm + '__' + file_db + '__' + str(arg_in.nump)  # .join(arg_in[:2]) #+ str_todos\n",
    "        NAME_FILE = ''.join(char for char in NAME_FILE if char in printable)\n",
    "        print(repr(NAME_FILE))\n",
    "\n",
    "        res_modelos = []\n",
    "        GA_models = []\n",
    "        historical_todos = []\n",
    "        tiempo_inicial = time.time()\n",
    "        outer_res = []\n",
    "\n",
    "        for num_seed in range(arg_in.numseeds):\n",
    "            print('Procesando Base de Datos con num_seed=', num_seed)\n",
    "            df_VAL = pd.read_csv(file_db)\n",
    "            print(df_VAL.shape)\n",
    "            train_val_size = df_VAL.shape[0]//2\n",
    "            if train_val_size>2000:\n",
    "                train_val_size = 2000\n",
    "            names_tiempo[2] = train_val_size\n",
    "            print(name_db, time_limit, train_val_size)\n",
    "            num_feats = df_VAL.shape[1]-1\n",
    "\n",
    "            df_VAL = pd.read_csv(file_db)\n",
    "            VAL = df_VAL.copy()\n",
    "            scaler_pruebas = StandardScaler()\n",
    "            VAL_norm = pd.DataFrame(scaler_pruebas.fit_transform(VAL), columns=VAL.columns)\n",
    "\n",
    "            TST_RMSE_ITER = []\n",
    "            NUM_FEATS = []\n",
    "\n",
    "            # Con selección\n",
    "            input_names = VAL_norm.columns[:-1]\n",
    "            target_name = VAL_norm.columns[-1]\n",
    "\n",
    "            X = VAL_norm[input_names]\n",
    "            y = VAL_norm[target_name]\n",
    "            seed_everything(num_seed)\n",
    "\n",
    "             # IMPORTANTE: En KFOLD_HYB_AUTOGLUON cojo 2000 filas de train_val y el resto de test...salvo que se especifique\n",
    "            # el parámetro train_size (que será el número de filas a pillar de la BD).\n",
    "            X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, \n",
    "                                                                        train_size = names_tiempo[2] / len(X), \n",
    "                                                                        random_state=12345)\n",
    "            size_train_val = len(X_train_val)\n",
    "            size_test = len(X_test)\n",
    "\n",
    "            model_name = 'KFOLD_HYB_AUTOGLUON'\n",
    "            selec_input_folds=[]\n",
    "            VAL_RMSE_ACC=[]\n",
    "            # En este caso, hacemos los X folds (fold es el numseeds)\n",
    "            kf = KFold(n_splits=arg_in.numseeds, shuffle=False)\n",
    "\n",
    "            file_name = [i for i in archivos if name_db in i and 'results' in i][0]\n",
    "            results_name = path_files + file_name\n",
    "            res_final = pd.read_csv(results_name) #DIR_SALIDA + '/' + NAME_FILE + '_results.csv')\n",
    "    #         display(res_final)\n",
    "\n",
    "            def convierte_str(cadena):\n",
    "                return np.array(eval(cadena.replace(' ',',').replace('\\n','').replace('nan','0')))\n",
    "\n",
    "\n",
    "            # FINAL MODEL FOR EACH OUTER WITH MEAN PARAMS\n",
    "            if arg_in.algorithm == 'mlp':\n",
    "                only_outer = res_final.query('num_seed==@num_seed').reset_index(drop=True)\n",
    "                only_outer['best_solution'] = only_outer['best_solution'].apply(lambda x: convierte_str(x))\n",
    "                only_outer['complex_best'] =  only_outer['best_solution'].apply(lambda x: x[2]).values\n",
    "                only_outer['hidden'] =  only_outer['best_solution'].apply(lambda x: x[3]).values\n",
    "                only_outer['alpha'] =  only_outer['best_solution'].apply(lambda x: 10**x[4]).values\n",
    "                only_outer['features'] =  only_outer['best_solution'].apply(lambda x: x[-len(input_names):]).values\n",
    "                mean_outer = only_outer[['VAL_RMSE_FOLD', 'VAL_MAE', 'VAL_MAPE', 'TST_RMSE_FOLD',\n",
    "                                         'TST_RMSE', 'TST_MAE', 'TST_MAPE',\n",
    "                                        'complex_best', 'hidden', 'alpha']].aggregate(['mean','std'])\n",
    "                fil0 = mean_outer.iloc[0]\n",
    "                fil0.index = ['mean_'+col for col in mean_outer.columns]\n",
    "                fil1 = mean_outer.iloc[1]\n",
    "                fil1.index = ['std_'+col for col in mean_outer.columns]\n",
    "                mean_outer = pd.concat(\n",
    "                    [only_outer[['method', 'database', 'namevar', 'size_train_val', 'size_test',\n",
    "                           'num_input_feats', 'num_seed', 'innerfold', 'run', 'model_name',\n",
    "                           'rerank_error', 'time', 'last_iter', 'mejor_val',\n",
    "                           'numindiv', 'maxiter', 'early', 'Lambda', 'c1', 'c2', 'IWmax', 'IWmin',\n",
    "                           'K', 'globthr', 'CVrep', 'CVspl', 'nruns', 'pcrossover_elitists',\n",
    "                           'pcrossover', 'best_solution']].head(1),\n",
    "                     pd.DataFrame(pd.concat([fil0,fil1])).transpose()],axis=1)\n",
    "\n",
    "                # Model with mean of hyperparameters\n",
    "                # Entrenamos con TRAIN+VAL y medimos el error con TST\n",
    "                model = MLPRegressor()\n",
    "                model.hidden_layer_sizes = int(mean_outer['mean_hidden'].values[0])\n",
    "                model.alpha = mean_outer['mean_alpha'].values[0]\n",
    "\n",
    "            if arg_in.algorithm == 'kridge':\n",
    "                only_outer = res_final.query('num_seed==@num_seed').reset_index(drop=True)\n",
    "                only_outer['best_solution'] = only_outer['best_solution'].apply(lambda x: convierte_str(x))\n",
    "                only_outer['complex_best'] =  only_outer['best_solution'].apply(lambda x: x[2]).values\n",
    "                only_outer['alpha'] =  only_outer['best_solution'].apply(lambda x: 10**x[3]).values\n",
    "                only_outer['gamma'] =  only_outer['best_solution'].apply(lambda x: 10**x[4]).values\n",
    "                only_outer['features'] =  only_outer['best_solution'].apply(lambda x: x[-len(input_names):]).values\n",
    "                mean_outer = only_outer[['VAL_RMSE_FOLD', 'VAL_MAE', 'VAL_MAPE', 'TST_RMSE_FOLD',\n",
    "                                         'TST_RMSE', 'TST_MAE', 'TST_MAPE',\n",
    "                                        'complex_best', 'alpha', 'gamma']].aggregate(['mean','std'])\n",
    "                fil0 = mean_outer.iloc[0]\n",
    "                fil0.index = ['mean_'+col for col in mean_outer.columns]\n",
    "                fil1 = mean_outer.iloc[1]\n",
    "                fil1.index = ['std_'+col for col in mean_outer.columns]\n",
    "                mean_outer = pd.concat(\n",
    "                    [only_outer[['method', 'database', 'namevar', 'size_train_val', 'size_test',\n",
    "                           'num_input_feats', 'num_seed', 'innerfold', 'run', 'model_name',\n",
    "                           'rerank_error', 'time', 'last_iter', 'mejor_val',\n",
    "                           'numindiv', 'maxiter', 'early', 'Lambda', 'c1', 'c2', 'IWmax', 'IWmin',\n",
    "                           'K', 'globthr', 'CVrep', 'CVspl', 'nruns', 'pcrossover_elitists',\n",
    "                           'pcrossover', 'best_solution']].head(1),\n",
    "                     pd.DataFrame(pd.concat([fil0,fil1])).transpose()],axis=1)\n",
    "\n",
    "                # Model with mean of hyperparameters\n",
    "                # Entrenamos con TRAIN+VAL y medimos el error con TST\n",
    "                model = res_search_model.best_model\n",
    "                model.alpha = mean_outer['mean_alpha'].values[0]\n",
    "                model.gamma = mean_outer['mean_gamma'].values[0]\n",
    "\n",
    "    #         selec_features = input_names[(np.stack(only_outer['features'].values)>0.50).mean(axis=0)>=0.50]\n",
    "            selec_features = input_names[(np.mean(np.stack(only_outer['features'].values),axis=0)>= thr_features)]\n",
    "            print(f'DB={name_db} train_val_size={train_val_size} ncols={len(input_names)} NFS={len(selec_features)}')\n",
    "            features_list.append(dict(name_db=name_db,\n",
    "                                      num_feats=num_feats,\n",
    "                                      num_selec = len(selec_features),\n",
    "\n",
    "                                      file_db=file_db,\n",
    "                                      results_name=results_name,\n",
    "                                      selec_features=np.array(selec_features)\n",
    "                                     ))\n",
    "\n",
    "    features_list = pd.DataFrame(features_list)\n",
    "\n",
    "    # Search with Bayesian\n",
    "    arg_in.maxiter = 200\n",
    "    run = 0\n",
    "    res = []\n",
    "    for names_tiempo in tiempos:\n",
    "        name_db = names_tiempo[0].split('_')[0]\n",
    "        list_csv = os.listdir('.')\n",
    "        list_csv = [i for i in list_csv if f'{name_db}' in i]\n",
    "        file_db = list_csv[0]\n",
    "        time_limit = int(names_tiempo[1] * 60)  # Time limit in secs\n",
    "        train_val_size = names_tiempo[2]\n",
    "        print(name_db, time_limit, train_val_size)\n",
    "\n",
    "        NAME_FILE = dt_string + '_' + arg_in.method + '__' + arg_in.algorithm + '__' + file_db + '__' + str(arg_in.nump)  # .join(arg_in[:2]) #+ str_todos\n",
    "        NAME_FILE = ''.join(char for char in NAME_FILE if char in printable)\n",
    "        print(repr(NAME_FILE))\n",
    "\n",
    "        res_modelos = []\n",
    "        GA_models = []\n",
    "        historical_todos = []\n",
    "        tiempo_inicial = time.time()\n",
    "        outer_res = []\n",
    "\n",
    "        for num_seed in range(arg_in.numseeds):\n",
    "            print('Procesando Base de Datos=', name_db,' con num_seed=', num_seed, 'thr_features=', thr_features)\n",
    "            df_VAL = pd.read_csv(file_db)\n",
    "            print(df_VAL.shape)\n",
    "            train_val_size = df_VAL.shape[0]//2\n",
    "            if train_val_size>2000:\n",
    "                train_val_size = 2000\n",
    "            names_tiempo[2] = train_val_size\n",
    "\n",
    "            num_feats = df_VAL.shape[1]-1\n",
    "\n",
    "            df_VAL = pd.read_csv(file_db)\n",
    "            VAL = df_VAL.copy()\n",
    "            scaler_pruebas = StandardScaler()\n",
    "            VAL_norm = pd.DataFrame(scaler_pruebas.fit_transform(VAL), columns=VAL.columns)\n",
    "\n",
    "            TST_RMSE_ITER = []\n",
    "            NUM_FEATS = []\n",
    "\n",
    "            # Con selección\n",
    "    #         input_names = VAL_norm.columns[:-1]\n",
    "            input_names = features_list.query('name_db==@name_db').iloc[num_seed]['selec_features']\n",
    "            target_name = VAL_norm.columns[-1]\n",
    "            print(f'DB={name_db} train_val_size={train_val_size} ncols={df_VAL.shape[1]-1} NFS={len(input_names)}')\n",
    "            if len(input_names)==0:\n",
    "                input_names = VAL_norm.columns[:2]\n",
    "            X = VAL_norm[input_names]\n",
    "            y = VAL_norm[target_name]\n",
    "            seed_everything(num_seed)\n",
    "\n",
    "            # IMPORTANTE: En KFOLD_HYB_AUTOGLUON cojo 2000 filas de train_val y el resto de test...salvo que se especifique\n",
    "            # el parámetro train_size (que será el número de filas a pillar de la BD).\n",
    "            X_train_val, X_test, y_train_val, y_test = train_test_split(X[input_names], y, \n",
    "                                                                        train_size = names_tiempo[2] / len(X), \n",
    "                                                                        random_state=12345)\n",
    "            size_train_val = len(X_train_val)\n",
    "            size_test = len(X_test)\n",
    "\n",
    "            model_name = 'KFOLD_HYB_AUTOGLUON'\n",
    "            selec_input_folds=[]\n",
    "            VAL_RMSE_ACC=[]\n",
    "\n",
    "            # Search With CV\n",
    "            # --------------\n",
    "            pbounds = dict()\n",
    "            for parm in params:\n",
    "                if params[parm]['type'] <= 1:\n",
    "                    pbounds[parm] = params[parm]['range']\n",
    "\n",
    "            tic = time.time()\n",
    "            optimizer = BayesianOptimization(f=optimize_fun,\n",
    "                                             pbounds=pbounds,\n",
    "                                             random_state=1234,\n",
    "                                             allow_duplicate_points=True) # Añado allow_duplicate_points=True para evitar errores que salían.\n",
    "            optimizer.maximize(init_points=10, n_iter=arg_in.maxiter - 10)\n",
    "            # Keep elapsed time in minutes\n",
    "            tac = time.time()\n",
    "            elapsed_time = (tac - tic) / 60.0\n",
    "            print('Elapsed_time:', elapsed_time)\n",
    "            print('Best:', optimizer.max)\n",
    "\n",
    "            if arg_in.algorithm == 'mlp':\n",
    "                # Best model\n",
    "                hidden_layer_sizes = optimizer.max['params']['hidden_layer_sizes']\n",
    "                alpha = 10.0**optimizer.max['params']['alpha']\n",
    "                params_opt = dict(hidden_layer_sizes=int(hidden_layer_sizes),\n",
    "                                  alpha=alpha,\n",
    "                                  solver='lbfgs',\n",
    "                                  activation='logistic',\n",
    "                                  n_iter_no_change=20,\n",
    "                                  tol=1e-5,\n",
    "                                  random_state=1234,\n",
    "                                  max_iter=5000)\n",
    "                params_indiv = [optimizer.max['params']['hidden_layer_sizes'], optimizer.max['params']['alpha']]\n",
    "                model = MLPRegressor(**params_opt)\n",
    "            \n",
    "                # Entrena con innerfolds y saca el error con test\n",
    "                model.fit(X_train_val[input_names].values, y_train_val.values)\n",
    "                y_test_pred = model.predict(X_test[input_names].values)\n",
    "                OUTER_RMSE = mean_squared_error(y_test.values, y_test_pred, squared=False)\n",
    "                OUTER_MAE = mean_absolute_error(y_test.values, y_test_pred)\n",
    "                OUTER_MAPE = mean_absolute_percentage_error(y_test.values, y_test_pred) * 100\n",
    "                print(OUTER_RMSE, OUTER_MAE, OUTER_MAPE)\n",
    "                res.append(dict(name_db=name_db,\n",
    "                                CV_RMSE = -optimizer.max['target'],\n",
    "                                OUTER_RMSE=OUTER_RMSE,\n",
    "                                OUTER_MAE=OUTER_MAE,\n",
    "                                OUTER_MAPE=OUTER_MAPE,\n",
    "                                train_val_size=train_val_size,\n",
    "                                test_size=len(X_test),\n",
    "                                ncols=df_VAL.shape[1]-1,\n",
    "                                NFs=len(input_names),\n",
    "                                hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                alpha=alpha,\n",
    "                                num_seed=num_seed,\n",
    "                                input_names=input_names))\n",
    "\n",
    "            if arg_in.algorithm == 'kridge':\n",
    "                # Best model\n",
    "                alpha = 10.0**optimizer.max['params']['alpha']\n",
    "                gamma = 10.0**optimizer.max['params']['gamma']\n",
    "                params_opt = dict(alpha=alpha, \n",
    "                                  gamma=gamma, \n",
    "                                  kernel=\"rbf\")\n",
    "                params_indiv = [optimizer.max['params']['alpha'], optimizer.max['params']['gamma']]\n",
    "                model = KernelRidge(**params_opt)\n",
    "                \n",
    "                # Entrena con innerfolds y saca el error con test\n",
    "                model.fit(X_train_val[input_names].values, y_train_val.values)\n",
    "                y_test_pred = model.predict(X_test[input_names].values)\n",
    "                OUTER_RMSE = mean_squared_error(y_test.values, y_test_pred, squared=False)\n",
    "                OUTER_MAE = mean_absolute_error(y_test.values, y_test_pred)\n",
    "                OUTER_MAPE = mean_absolute_percentage_error(y_test.values, y_test_pred) * 100\n",
    "                print(OUTER_RMSE, OUTER_MAE, OUTER_MAPE)\n",
    "                res.append(dict(name_db=name_db,\n",
    "                                CV_RMSE = -optimizer.max['target'],\n",
    "                                OUTER_RMSE=OUTER_RMSE,\n",
    "                                OUTER_MAE=OUTER_MAE,\n",
    "                                OUTER_MAPE=OUTER_MAPE,\n",
    "                                train_val_size=train_val_size,\n",
    "                                test_size=len(X_test),\n",
    "                                ncols=df_VAL.shape[1]-1,\n",
    "                                NFs=len(input_names),\n",
    "                                alpha=alpha,\n",
    "                                gamma=gamma,\n",
    "                                num_seed=num_seed,\n",
    "                                input_names=input_names))\n",
    "            res_total = pd.DataFrame(res)\n",
    "            res_total.to_csv(f'res_bayes_after_pso_thrfeats_{thr_features}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426794f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T11:49:32.895871Z",
     "start_time": "2023-04-16T11:49:32.721325Z"
    }
   },
   "outputs": [],
   "source": [
    "res_total.groupby('name_db').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbddccd5",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f369bae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T12:18:02.228537Z",
     "start_time": "2023-04-16T12:16:51.668903Z"
    }
   },
   "outputs": [],
   "source": [
    "features_list_thr = []\n",
    "for thr_features in [0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70]:\n",
    "    run = 0\n",
    "    for names_tiempo in tiempos:\n",
    "        name_db = names_tiempo[0].split('_')[0]\n",
    "        list_csv = os.listdir('.')\n",
    "        list_csv = [i for i in list_csv if f'{name_db}' in i]\n",
    "        file_db = list_csv[0]\n",
    "        time_limit = int(names_tiempo[1] * 60)  # Time limit in secs\n",
    "        train_val_size = names_tiempo[2]\n",
    "        print(name_db, time_limit, train_val_size)\n",
    "\n",
    "        NAME_FILE = dt_string + '_' + arg_in.method + '__' + arg_in.algorithm + '__' + file_db + '__' + str(arg_in.nump)  # .join(arg_in[:2]) #+ str_todos\n",
    "        NAME_FILE = ''.join(char for char in NAME_FILE if char in printable)\n",
    "        print(repr(NAME_FILE))\n",
    "\n",
    "        res_modelos = []\n",
    "        GA_models = []\n",
    "        historical_todos = []\n",
    "        tiempo_inicial = time.time()\n",
    "        outer_res = []\n",
    "\n",
    "        for num_seed in range(arg_in.numseeds):\n",
    "            print('Procesando Base de Datos con num_seed=', num_seed)\n",
    "            df_VAL = pd.read_csv(file_db)\n",
    "            print(df_VAL.shape)\n",
    "            train_val_size = df_VAL.shape[0]//2\n",
    "            if train_val_size>2000:\n",
    "                train_val_size = 2000\n",
    "            names_tiempo[2] = train_val_size\n",
    "            print(name_db, time_limit, train_val_size)\n",
    "            num_feats = df_VAL.shape[1]-1\n",
    "\n",
    "            df_VAL = pd.read_csv(file_db)\n",
    "            VAL = df_VAL.copy()\n",
    "            scaler_pruebas = StandardScaler()\n",
    "            VAL_norm = pd.DataFrame(scaler_pruebas.fit_transform(VAL), columns=VAL.columns)\n",
    "\n",
    "            TST_RMSE_ITER = []\n",
    "            NUM_FEATS = []\n",
    "\n",
    "            # Con selección\n",
    "            input_names = VAL_norm.columns[:-1]\n",
    "            target_name = VAL_norm.columns[-1]\n",
    "\n",
    "            X = VAL_norm[input_names]\n",
    "            y = VAL_norm[target_name]\n",
    "            seed_everything(num_seed)\n",
    "\n",
    "             # IMPORTANTE: En KFOLD_HYB_AUTOGLUON cojo 2000 filas de train_val y el resto de test...salvo que se especifique\n",
    "            # el parámetro train_size (que será el número de filas a pillar de la BD).\n",
    "            X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, \n",
    "                                                                        train_size = names_tiempo[2] / len(X), \n",
    "                                                                        random_state=12345)\n",
    "            size_train_val = len(X_train_val)\n",
    "            size_test = len(X_test)\n",
    "\n",
    "            model_name = 'KFOLD_HYB_AUTOGLUON'\n",
    "            selec_input_folds=[]\n",
    "            VAL_RMSE_ACC=[]\n",
    "            # En este caso, hacemos los X folds (fold es el numseeds)\n",
    "            kf = KFold(n_splits=arg_in.numseeds, shuffle=False)\n",
    "\n",
    "            file_name = [i for i in archivos if name_db in i and 'results' in i][0]\n",
    "            results_name = path_files + file_name\n",
    "            res_final = pd.read_csv(results_name) #DIR_SALIDA + '/' + NAME_FILE + '_results.csv')\n",
    "    #         display(res_final)\n",
    "\n",
    "            def convierte_str(cadena):\n",
    "                return np.array(eval(cadena.replace(' ',',').replace('\\n','').replace('nan','0')))\n",
    "\n",
    "\n",
    "            # FINAL MODEL FOR EACH OUTER WITH MEAN PARAMS\n",
    "            if arg_in.algorithm == 'mlp':\n",
    "                only_outer = res_final.query('num_seed==@num_seed').reset_index(drop=True)\n",
    "                only_outer['best_solution'] = only_outer['best_solution'].apply(lambda x: convierte_str(x))\n",
    "                only_outer['complex_best'] =  only_outer['best_solution'].apply(lambda x: x[2]).values\n",
    "                only_outer['hidden'] =  only_outer['best_solution'].apply(lambda x: x[3]).values\n",
    "                only_outer['alpha'] =  only_outer['best_solution'].apply(lambda x: 10**x[4]).values\n",
    "                only_outer['features'] =  only_outer['best_solution'].apply(lambda x: x[-len(input_names):]).values\n",
    "                mean_outer = only_outer[['VAL_RMSE_FOLD', 'VAL_MAE', 'VAL_MAPE', 'TST_RMSE_FOLD',\n",
    "                                         'TST_RMSE', 'TST_MAE', 'TST_MAPE',\n",
    "                                        'complex_best', 'hidden', 'alpha']].aggregate(['mean','std'])\n",
    "                fil0 = mean_outer.iloc[0]\n",
    "                fil0.index = ['mean_'+col for col in mean_outer.columns]\n",
    "                fil1 = mean_outer.iloc[1]\n",
    "                fil1.index = ['std_'+col for col in mean_outer.columns]\n",
    "                mean_outer = pd.concat(\n",
    "                    [only_outer[['method', 'database', 'namevar', 'size_train_val', 'size_test',\n",
    "                           'num_input_feats', 'num_seed', 'innerfold', 'run', 'model_name',\n",
    "                           'rerank_error', 'time', 'last_iter', 'mejor_val',\n",
    "                           'numindiv', 'maxiter', 'early', 'Lambda', 'c1', 'c2', 'IWmax', 'IWmin',\n",
    "                           'K', 'globthr', 'CVrep', 'CVspl', 'nruns', 'pcrossover_elitists',\n",
    "                           'pcrossover', 'best_solution']].head(1),\n",
    "                     pd.DataFrame(pd.concat([fil0,fil1])).transpose()],axis=1)\n",
    "\n",
    "                # Model with mean of hyperparameters\n",
    "                # Entrenamos con TRAIN+VAL y medimos el error con TST\n",
    "                model = MLPRegressor()\n",
    "                model.hidden_layer_sizes = int(mean_outer['mean_hidden'].values[0])\n",
    "                model.alpha = mean_outer['mean_alpha'].values[0]\n",
    "\n",
    "            if arg_in.algorithm == 'kridge':\n",
    "                only_outer = res_final.query('num_seed==@num_seed').reset_index(drop=True)\n",
    "                only_outer['best_solution'] = only_outer['best_solution'].apply(lambda x: convierte_str(x))\n",
    "                only_outer['complex_best'] =  only_outer['best_solution'].apply(lambda x: x[2]).values\n",
    "                only_outer['alpha'] =  only_outer['best_solution'].apply(lambda x: 10**x[3]).values\n",
    "                only_outer['gamma'] =  only_outer['best_solution'].apply(lambda x: 10**x[4]).values\n",
    "                only_outer['features'] =  only_outer['best_solution'].apply(lambda x: x[-len(input_names):]).values\n",
    "                mean_outer = only_outer[['VAL_RMSE_FOLD', 'VAL_MAE', 'VAL_MAPE', 'TST_RMSE_FOLD',\n",
    "                                         'TST_RMSE', 'TST_MAE', 'TST_MAPE',\n",
    "                                        'complex_best', 'alpha', 'gamma']].aggregate(['mean','std'])\n",
    "                fil0 = mean_outer.iloc[0]\n",
    "                fil0.index = ['mean_'+col for col in mean_outer.columns]\n",
    "                fil1 = mean_outer.iloc[1]\n",
    "                fil1.index = ['std_'+col for col in mean_outer.columns]\n",
    "                mean_outer = pd.concat(\n",
    "                    [only_outer[['method', 'database', 'namevar', 'size_train_val', 'size_test',\n",
    "                           'num_input_feats', 'num_seed', 'innerfold', 'run', 'model_name',\n",
    "                           'rerank_error', 'time', 'last_iter', 'mejor_val',\n",
    "                           'numindiv', 'maxiter', 'early', 'Lambda', 'c1', 'c2', 'IWmax', 'IWmin',\n",
    "                           'K', 'globthr', 'CVrep', 'CVspl', 'nruns', 'pcrossover_elitists',\n",
    "                           'pcrossover', 'best_solution']].head(1),\n",
    "                     pd.DataFrame(pd.concat([fil0,fil1])).transpose()],axis=1)\n",
    "\n",
    "                # Model with mean of hyperparameters\n",
    "                # Entrenamos con TRAIN+VAL y medimos el error con TST\n",
    "                model = res_search_model.best_model\n",
    "                model.alpha = mean_outer['mean_alpha'].values[0]\n",
    "                model.gamma = mean_outer['mean_gamma'].values[0]\n",
    "\n",
    "    #         selec_features = input_names[(np.stack(only_outer['features'].values)>0.50).mean(axis=0)>=0.50]\n",
    "            selec_features = input_names[(np.mean(np.stack(only_outer['features'].values),axis=0)>= thr_features)]\n",
    "            print(f'DB={name_db} train_val_size={train_val_size} ncols={len(input_names)} NFS={len(selec_features)}')\n",
    "            features_list_thr.append(dict(name_db=name_db,\n",
    "                                          num_seed = num_seed,\n",
    "                                          thr_features=thr_features,\n",
    "                                          num_feats=num_feats,\n",
    "                                          mean_last_iter = mean_outer['last_iter'].values[0],\n",
    "                                          num_selec = len(selec_features),\n",
    "\n",
    "                                          file_db=file_db,\n",
    "                                          results_name=results_name,\n",
    "                                          selec_features=np.array(selec_features)\n",
    "                                     ))\n",
    "\n",
    "features_list_thr = pd.DataFrame(features_list_thr)\n",
    "features_list_thr.groupby(['name_db','thr_features']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1176d7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T14:50:39.888241Z",
     "start_time": "2023-04-17T14:50:39.843419Z"
    }
   },
   "outputs": [],
   "source": [
    "features_list_thr_mean = features_list_thr.groupby(['name_db','thr_features']).mean().reset_index()\n",
    "features_list_thr_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38fc5e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T14:53:36.628292Z",
     "start_time": "2023-04-17T14:53:36.600876Z"
    }
   },
   "outputs": [],
   "source": [
    "features_list_thr_mean.query('thr_features==@thr_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba132f2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T14:54:48.936840Z",
     "start_time": "2023-04-17T14:54:48.851504Z"
    }
   },
   "outputs": [],
   "source": [
    "for thr_features in [0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70]:\n",
    "    res_total = pd.read_csv(f'res_bayes_after_pso_thrfeats_{thr_features}.csv')\n",
    "    mean_res = res_total.groupby('name_db').mean().reset_index()\n",
    "    if thr_features == 0.00:\n",
    "        db_mean = mean_res[['name_db', 'train_val_size', 'test_size', 'ncols']]\n",
    "        db_mean.rename({'OUTER_RMSE':'RMSE_0.0'}, inplace=True)\n",
    "        db_mean['last_iter'] = features_list_thr_mean.query('thr_features==@thr_features')['mean_last_iter'].values\n",
    "    db_mean[f'RMSE_{thr_features}'] = mean_res['OUTER_RMSE']\n",
    "    db_mean[f'NFs_{thr_features}'] = features_list_thr_mean.query('thr_features==@thr_features')['num_selec'].values\n",
    "db_mean                   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355e9e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T14:55:24.606731Z",
     "start_time": "2023-04-17T14:55:24.557027Z"
    }
   },
   "outputs": [],
   "source": [
    "print(db_mean.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06517a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e72f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T16:45:33.923784Z",
     "start_time": "2023-04-17T16:45:33.886695Z"
    }
   },
   "outputs": [],
   "source": [
    "features_list_thr_std = features_list_thr.groupby(['name_db','thr_features']).std().reset_index()\n",
    "features_list_thr_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf9d06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T16:46:19.855239Z",
     "start_time": "2023-04-17T16:46:19.770520Z"
    }
   },
   "outputs": [],
   "source": [
    "for thr_features in [0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70]:\n",
    "    res_total = pd.read_csv(f'res_bayes_after_pso_thrfeats_{thr_features}.csv')\n",
    "    std_res = res_total.groupby('name_db').std().reset_index()\n",
    "    if thr_features == 0.00:\n",
    "        db_std = std_res[['name_db', 'train_val_size', 'test_size', 'ncols']]\n",
    "        db_std.rename({'OUTER_RMSE':'RMSE_0.0'}, inplace=True)\n",
    "        db_std['last_iter'] = features_list_thr_std.query('thr_features==@thr_features')['mean_last_iter'].values\n",
    "    db_std[f'RMSE_{thr_features}'] = std_res['OUTER_RMSE']\n",
    "    db_std[f'NFs_{thr_features}'] = features_list_thr_std.query('thr_features==@thr_features')['num_selec'].values\n",
    "db_std                   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ab948",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T16:48:19.888168Z",
     "start_time": "2023-04-17T16:48:19.867216Z"
    }
   },
   "outputs": [],
   "source": [
    "db_std[['name_db', 'RMSE_0.5', 'NFs_0.5']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
